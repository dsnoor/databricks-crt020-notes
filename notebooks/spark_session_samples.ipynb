{"cells":[{"cell_type":"markdown","source":["# Spark Session\n\nThis notebook covers the Spark Session topics provided in the certification summary"],"metadata":{}},{"cell_type":"markdown","source":["# Create a DataFrame/Dataset from a collection (e.g. list or set)\n\nThis notebook contain sample code with various ways to generate a Spark Dataframe from python"],"metadata":{}},{"cell_type":"markdown","source":["## Creating Dataframes\n\nSpark Session has a method called [createDataFrame](https://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html#pyspark.sql.SparkSession.createDataFrame), which receive data and convert it to a dataframe. This method params are listed below:\n\n<br>\n`SparkSession.createDataFrame(data, schema=None, samplingRatio=None, verifySchema=True)` \n\n<br>\n\n* `data`: Data that will populate a Dataframe, you can pass a python list, a pandas dataframe a numpy array or fuck it\n* `schema(opt)`: Data types for the Dataframe, if you don't input any schema, spark will try to infer the schema from `data`\n* `samplingRatio(opt)`: Control the amount of data used on schema infering, may be good to use with big Datasets\n* `verifySchema(opt)`: Validate the input schema for the entire Dataframe"],"metadata":{}},{"cell_type":"markdown","source":["## Creating a dataframe from a python list\n\nOkay, let's create a spark dataframe from a single list\n\nWhen creating a dataframe from a list, you need to have a nested list structure to have rows, a single list will represent a single row on a Dataframe"],"metadata":{}},{"cell_type":"code","source":["#creating a spark dataframe from a python list\n\nmy_list = [1,2,3,4,5,6,7,8]\n\nmy_df = spark.createDataFrame(my_list)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">---------------------------------------------------------------------------</span>\n<span class=\"ansired\">TypeError</span>                                 Traceback (most recent call last)\n<span class=\"ansigreen\">&lt;command-3364551252242354&gt;</span> in <span class=\"ansicyan\">&lt;module&gt;</span><span class=\"ansiblue\">()</span>\n<span class=\"ansigreen\">      3</span> my_list <span class=\"ansiyellow\">=</span> <span class=\"ansiyellow\">[</span><span class=\"ansicyan\">1</span><span class=\"ansiyellow\">,</span><span class=\"ansicyan\">2</span><span class=\"ansiyellow\">,</span><span class=\"ansicyan\">3</span><span class=\"ansiyellow\">,</span><span class=\"ansicyan\">4</span><span class=\"ansiyellow\">,</span><span class=\"ansicyan\">5</span><span class=\"ansiyellow\">,</span><span class=\"ansicyan\">6</span><span class=\"ansiyellow\">,</span><span class=\"ansicyan\">7</span><span class=\"ansiyellow\">,</span><span class=\"ansicyan\">8</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">      4</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">----&gt; 5</span><span class=\"ansiyellow\"> </span>my_df <span class=\"ansiyellow\">=</span> spark<span class=\"ansiyellow\">.</span>createDataFrame<span class=\"ansiyellow\">(</span>my_list<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/sql/session.py</span> in <span class=\"ansicyan\">createDataFrame</span><span class=\"ansiblue\">(self, data, schema, samplingRatio, verifySchema)</span>\n<span class=\"ansigreen\">    815</span>                 rdd<span class=\"ansiyellow\">,</span> schema <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>_createFromRDD<span class=\"ansiyellow\">(</span>data<span class=\"ansiyellow\">.</span>map<span class=\"ansiyellow\">(</span>prepare<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">,</span> schema<span class=\"ansiyellow\">,</span> samplingRatio<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    816</span>             <span class=\"ansigreen\">else</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 817</span><span class=\"ansiyellow\">                 </span>rdd<span class=\"ansiyellow\">,</span> schema <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>_createFromLocal<span class=\"ansiyellow\">(</span>map<span class=\"ansiyellow\">(</span>prepare<span class=\"ansiyellow\">,</span> data<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">,</span> schema<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    818</span>             jrdd <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>_jvm<span class=\"ansiyellow\">.</span>SerDeUtil<span class=\"ansiyellow\">.</span>toJavaArray<span class=\"ansiyellow\">(</span>rdd<span class=\"ansiyellow\">.</span>_to_java_object_rdd<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    819</span>             jdf <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>_jsparkSession<span class=\"ansiyellow\">.</span>applySchemaToPythonRDD<span class=\"ansiyellow\">(</span>jrdd<span class=\"ansiyellow\">.</span>rdd<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">,</span> schema<span class=\"ansiyellow\">.</span>json<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/sql/session.py</span> in <span class=\"ansicyan\">_createFromLocal</span><span class=\"ansiblue\">(self, data, schema)</span>\n<span class=\"ansigreen\">    440</span>         write temp files<span class=\"ansiyellow\">.</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    441</span>         &quot;&quot;&quot;\n<span class=\"ansigreen\">--&gt; 442</span><span class=\"ansiyellow\">         </span>data<span class=\"ansiyellow\">,</span> schema <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>_wrap_data_schema<span class=\"ansiyellow\">(</span>data<span class=\"ansiyellow\">,</span> schema<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    443</span>         <span class=\"ansigreen\">return</span> self<span class=\"ansiyellow\">.</span>_sc<span class=\"ansiyellow\">.</span>parallelize<span class=\"ansiyellow\">(</span>data<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">,</span> schema<span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    444</span> <span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/sql/session.py</span> in <span class=\"ansicyan\">_wrap_data_schema</span><span class=\"ansiblue\">(self, data, schema)</span>\n<span class=\"ansigreen\">    419</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    420</span>         <span class=\"ansigreen\">if</span> schema <span class=\"ansigreen\">is</span> <span class=\"ansigreen\">None</span> <span class=\"ansigreen\">or</span> isinstance<span class=\"ansiyellow\">(</span>schema<span class=\"ansiyellow\">,</span> <span class=\"ansiyellow\">(</span>list<span class=\"ansiyellow\">,</span> tuple<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 421</span><span class=\"ansiyellow\">             </span>struct <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>_inferSchemaFromList<span class=\"ansiyellow\">(</span>data<span class=\"ansiyellow\">,</span> names<span class=\"ansiyellow\">=</span>schema<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    422</span>             converter <span class=\"ansiyellow\">=</span> _create_converter<span class=\"ansiyellow\">(</span>struct<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    423</span>             data <span class=\"ansiyellow\">=</span> map<span class=\"ansiyellow\">(</span>converter<span class=\"ansiyellow\">,</span> data<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/sql/session.py</span> in <span class=\"ansicyan\">_inferSchemaFromList</span><span class=\"ansiblue\">(self, data, names)</span>\n<span class=\"ansigreen\">    355</span>             warnings.warn(&quot;inferring schema from dict is deprecated,&quot;\n<span class=\"ansigreen\">    356</span>                           &quot;please use pyspark.sql.Row instead&quot;)\n<span class=\"ansigreen\">--&gt; 357</span><span class=\"ansiyellow\">         </span>schema <span class=\"ansiyellow\">=</span> reduce<span class=\"ansiyellow\">(</span>_merge_type<span class=\"ansiyellow\">,</span> <span class=\"ansiyellow\">(</span>_infer_schema<span class=\"ansiyellow\">(</span>row<span class=\"ansiyellow\">,</span> names<span class=\"ansiyellow\">)</span> <span class=\"ansigreen\">for</span> row <span class=\"ansigreen\">in</span> data<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    358</span>         <span class=\"ansigreen\">if</span> _has_nulltype<span class=\"ansiyellow\">(</span>schema<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    359</span>             <span class=\"ansigreen\">raise</span> ValueError<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&quot;Some of types cannot be determined after inferring&quot;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/sql/session.py</span> in <span class=\"ansicyan\">&lt;genexpr&gt;</span><span class=\"ansiblue\">(.0)</span>\n<span class=\"ansigreen\">    355</span>             warnings.warn(&quot;inferring schema from dict is deprecated,&quot;\n<span class=\"ansigreen\">    356</span>                           &quot;please use pyspark.sql.Row instead&quot;)\n<span class=\"ansigreen\">--&gt; 357</span><span class=\"ansiyellow\">         </span>schema <span class=\"ansiyellow\">=</span> reduce<span class=\"ansiyellow\">(</span>_merge_type<span class=\"ansiyellow\">,</span> <span class=\"ansiyellow\">(</span>_infer_schema<span class=\"ansiyellow\">(</span>row<span class=\"ansiyellow\">,</span> names<span class=\"ansiyellow\">)</span> <span class=\"ansigreen\">for</span> row <span class=\"ansigreen\">in</span> data<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    358</span>         <span class=\"ansigreen\">if</span> _has_nulltype<span class=\"ansiyellow\">(</span>schema<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    359</span>             <span class=\"ansigreen\">raise</span> ValueError<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&quot;Some of types cannot be determined after inferring&quot;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/sql/types.py</span> in <span class=\"ansicyan\">_infer_schema</span><span class=\"ansiblue\">(row, names)</span>\n<span class=\"ansigreen\">   1060</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1061</span>     <span class=\"ansigreen\">else</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">-&gt; 1062</span><span class=\"ansiyellow\">         </span><span class=\"ansigreen\">raise</span> TypeError<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&quot;Can not infer schema for type: %s&quot;</span> <span class=\"ansiyellow\">%</span> type<span class=\"ansiyellow\">(</span>row<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1063</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1064</span>     fields <span class=\"ansiyellow\">=</span> <span class=\"ansiyellow\">[</span>StructField<span class=\"ansiyellow\">(</span>k<span class=\"ansiyellow\">,</span> _infer_type<span class=\"ansiyellow\">(</span>v<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">,</span> <span class=\"ansigreen\">True</span><span class=\"ansiyellow\">)</span> <span class=\"ansigreen\">for</span> k<span class=\"ansiyellow\">,</span> v <span class=\"ansigreen\">in</span> items<span class=\"ansiyellow\">]</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansired\">TypeError</span>: Can not infer schema for type: &lt;class &apos;int&apos;&gt;</div>"]}}],"execution_count":5},{"cell_type":"code","source":["import pyspark.sql.types as tp\nteste = spark.createDataFrame(my_list,schema=tp.IntegerType())"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":6},{"cell_type":"code","source":["display(teste)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>value</th></tr></thead><tbody><tr><td>1</td></tr><tr><td>2</td></tr><tr><td>3</td></tr><tr><td>4</td></tr><tr><td>5</td></tr><tr><td>6</td></tr><tr><td>7</td></tr><tr><td>8</td></tr></tbody></table></div>"]}}],"execution_count":7},{"cell_type":"code","source":["tp.StructType(\n  ['integer1':tp.IntegerType(), 'integer2':tp.IntegerType())"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":[" {'integer1':tp.IntegerType(), 'integer2':tp.IntegerType()}"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":9}],"metadata":{"name":"spark_session_samples","notebookId":3364551252242351},"nbformat":4,"nbformat_minor":0}

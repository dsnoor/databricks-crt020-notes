{"cells":[{"cell_type":"markdown","source":["# Read data with Spark\n\nThis notebook covers the DataFrameReader topics about how to read data, just to refresh:\n\nCandidates are expected to know how to:\n\n* Read data for the “core” data formats (CSV, JSON, JDBC, ORC, Parquet, text and tables)\n* How to configure options for specific formats\n* How to read data from non-core formats using format() and load()\n* How to specify a DDL-formatted schema\n* How to construct and specify a schema using the StructType classes"],"metadata":{}},{"cell_type":"markdown","source":["## Read data for the “core” data formats (CSV, JSON, JDBC, ORC, Parquet, text and tables)\n\nTo read data with Spark, you must use the [DataframeReader](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameReader) object, contained into the Spark Session, to acess the DataFrameReader in Databricks, just type `spark.read`"],"metadata":{}},{"cell_type":"code","source":["spark.read"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">4</span><span class=\"ansired\">]: </span>&lt;pyspark.sql.readwriter.DataFrameReader at 0x7effc86b62e8&gt;</div>"]}}],"execution_count":3},{"cell_type":"markdown","source":["if you run the cell above, it will display the `DataFrameReader`, but it won't work if you don't specify which type of data you want to read.\n\nThe \"core\" data formats are methods built in into `DataFrameReader`, that will have more rich features to handle data of that specific format. Let's read each type of data now."],"metadata":{}},{"cell_type":"markdown","source":["### Reading CSV data\n\nFirst we will read a CSV(comma separated values) file, which consists of a file(or a set of files) that have data **organized in rows** separated by a **delimiter**(which, for me, it's never a comma), may have a **header row** or not and at last, the data may have **enclosing quotes** or not."],"metadata":{}},{"cell_type":"code","source":["import glob\n\nglob.glob(\"/dbfs/databricks-datasets/*/*.csv\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">20</span><span class=\"ansired\">]: </span>[&apos;/dbfs/databricks-datasets/atlas_higgs/atlas_higgs.csv&apos;,\n &apos;/dbfs/databricks-datasets/flights/departuredelays.csv&apos;,\n &apos;/dbfs/databricks-datasets/learning-spark-v2/sf-fire-calls.csv&apos;,\n &apos;/dbfs/databricks-datasets/lending-club-loan-stats/LoanStats_2018Q2.csv&apos;,\n &apos;/dbfs/databricks-datasets/sai-summit-2019-sf/fire-calls.csv&apos;,\n &apos;/dbfs/databricks-datasets/sfo_customer_survey/2013_SFO_Customer_Survey.csv&apos;]</div>"]}}],"execution_count":6},{"cell_type":"markdown","source":["For this reads, we will use Databricks toy datasets located in `/databricks-datasets` when available. For this CSV exercise we got plenty of files, so we will pick `/dbfs/databricks-datasets/flights/departuredelays.csv`.\n\nLet's check how this file looks like:"],"metadata":{}},{"cell_type":"code","source":["%sh head -5 /dbfs/databricks-datasets/flights/departuredelays.csv"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">date,delay,distance,origin,destination\n01011245,6,602,ABE,ATL\n01020600,-8,369,ABE,DTW\n01021245,-2,602,ABE,ATL\n01020605,-4,602,ABE,ATL\n</div>"]}}],"execution_count":8},{"cell_type":"markdown","source":["it's a file delimited by commas(miracle!) with a header and no enclosing quotes. So, let's transform it into a Spark Dataframe with [read.csv](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameReader.csv)!"],"metadata":{}},{"cell_type":"code","source":["delay_filepath = '/databricks-datasets/flights/departuredelays.csv'\n\ndelays = spark.read.csv(delay_filepath)\n\ndisplay(delays.limit(5))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>_c0</th><th>_c1</th><th>_c2</th><th>_c3</th><th>_c4</th></tr></thead><tbody><tr><td>date</td><td>delay</td><td>distance</td><td>origin</td><td>destination</td></tr><tr><td>01011245</td><td>6</td><td>602</td><td>ABE</td><td>ATL</td></tr><tr><td>01020600</td><td>-8</td><td>369</td><td>ABE</td><td>DTW</td></tr><tr><td>01021245</td><td>-2</td><td>602</td><td>ABE</td><td>ATL</td></tr><tr><td>01020605</td><td>-4</td><td>602</td><td>ABE</td><td>ATL</td></tr></tbody></table></div>"]}}],"execution_count":10},{"cell_type":"markdown","source":["Nice, we just have created a `dataframe` from a csv file, but some things aren't quite right here:\n\n* The header were considered as a row on the read process\n* All fields are strings\n* The `date` field looks wierd as f**k\n\nTo fix this, you need to pass extra parameters to the `read.csv`, informing these caveats to Spark."],"metadata":{}},{"cell_type":"code","source":["#reading again, with extra params\n\ndelays_done_right = spark.read.csv(delay_filepath,inferSchema=True,header=True)\n\ndisplay(delays_done_right.limit(3))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>date</th><th>delay</th><th>distance</th><th>origin</th><th>destination</th></tr></thead><tbody><tr><td>1011245</td><td>6</td><td>602</td><td>ABE</td><td>ATL</td></tr><tr><td>1020600</td><td>-8</td><td>369</td><td>ABE</td><td>DTW</td></tr><tr><td>1021245</td><td>-2</td><td>602</td><td>ABE</td><td>ATL</td></tr></tbody></table></div>"]}}],"execution_count":12},{"cell_type":"markdown","source":["Almost everything done, the column names were specified by the `header=True` and the data types were inferred by `inferSchema=True`.\n\n> The date is still weird, that's because year was ommited from the data, but, this is a problem for another notebook\n\nTime to check another formats"],"metadata":{}}],"metadata":{"name":"reading_data","notebookId":1358552509597161},"nbformat":4,"nbformat_minor":0}
